{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# System de clusterisation de données web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liste des dépendance :\n",
    "* sklearn\n",
    "* numpy\n",
    "* collections\n",
    "* pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from random import shuffle\n",
    "\n",
    "import pickle\n",
    "\n",
    "#Debug\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture de fichier\n",
    "* Prend un chemin vers un fichier.\n",
    "  * Le fichier doit être au format élément séparteur vecteur ( sous la forme nombre séparateur nombre ... )\n",
    "  * Le séparateur est un espace\n",
    "  * La normalisation des vecteurs n'est pas nécessaire\n",
    "* Renvoie un tuple contenant la liste des éléments ainsi que leur représentation vectorielle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_vector_file(path_to_file):\n",
    "    keys = []\n",
    "    vectors = []\n",
    "    with open(path_to_file) as file :\n",
    "        line = file.readline()\n",
    "        while line :\n",
    "            data = line[:-1].split(\" \")\n",
    "            web, vector = data[0], [float(s) for s in data[1:]]\n",
    "            vectors.append(vector)\n",
    "            keys.append(web)\n",
    "            line = file.readline()\n",
    "    return (keys, vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions de préparation des données :\n",
    "* Prend en entrée, une liste de site, une de vecteurs associés, une de labels associés\n",
    "* Nettoie certaines catégories : \"Régional\"\n",
    "* Renvoie 2 dictionnaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_someting(websites, vectors, classed_file, verbose = False) :\n",
    "    site2ind_class = {}\n",
    "    ind2site = []\n",
    "    good_vecs = []\n",
    "    count = defaultdict(int)\n",
    "    keys = set(websites)\n",
    "    with open(classed_file) as file :\n",
    "            i = 0\n",
    "            line = file.readline()\n",
    "            while line :\n",
    "                cat, site = line[:-1].split(\"\\t\")\n",
    "                if site in keys :\n",
    "                    cat = cat.split(\"/\")[3]\n",
    "                    if site not in site2ind_class and cat != 'Régional':\n",
    "                        site2ind_class[site] = (i, cat)\n",
    "                        count[cat]+=1\n",
    "                        ind2site.append(site)\n",
    "                        good_vecs.append(vectors[websites.index(site)])\n",
    "                        i+=1\n",
    "                line = file.readline()\n",
    "    if verbose :\n",
    "        pp.pprint(count)\n",
    "    return site2ind_class, ind2site, good_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### VAR ZONE ####\n",
    "file_path = \"fr.up.seeds.txt.shuf.10000\" \n",
    "classed_file = \"dmozFull.fr\"\n",
    "nb_cluster = 10\n",
    "## END VAR ZONE ##\n",
    "\n",
    "#### EXEC ZONE ####\n",
    "websites, vectors = read_vector_file(file_path)\n",
    "site2ind_class, ind2site, good_vecs = do_someting(websites, vectors, classed_file)\n",
    "## END EXEC ZONE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering des données : approche k_means\n",
    "* Prend un nombre de cluster et une liste de vecteurs\n",
    "* Renvoie les clusters associés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_k_means(vectors, nb_clus):\n",
    "    kmeans = MiniBatchKMeans(n_clusters = nb_clus, \n",
    "                batch_size = 10000,\n",
    "                max_iter = 200)\n",
    "    classif = kmeans.fit_predict(vectors)\n",
    "    return classif, kmeans.cluster_centers_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering des données : approche HAC\n",
    "* Prend une liste de de vecteurs\n",
    "* Les mélanges aléatoirement\n",
    "* Initialise :\n",
    "    * En prend 200\n",
    "    * Calcule une matrice de similarité entre les 200\n",
    "    * Effectue une fusion\n",
    "        * Trouver le max\n",
    "        * Fusionner\n",
    "    * Recalcule la similarité\n",
    "    * Se souvient de la position supprimée.\n",
    "* Boucle :\n",
    "    * Prend un vecteur\n",
    "    * Calcul la similarité\n",
    "    * Effectue une fusion\n",
    "        * Trouver le max\n",
    "        * Fusionner\n",
    "    * Recalcule la similarité\n",
    "    * Se souvient de la position supprimée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HAC :\n",
    "    \n",
    "    def __init__(self, batch_size = 20, min_merge = 0.7, max_passe = 2, loss = 0):\n",
    "        \n",
    "        #Liste de cluster à accès rapide = dic: int->set\n",
    "        self.clusters = {}\n",
    "        \n",
    "        #Similarité minimale pour fusionner = float\n",
    "        self.min_merge = min_merge\n",
    "        \n",
    "        #Nombre de passes = int\n",
    "        self.max_passe = max_passe\n",
    "        \n",
    "        #Facteur de diminution de la valeur minimale de fusion = float\n",
    "        self.loss = loss\n",
    "        \n",
    "        #Matrice de similarité = list of list of int\n",
    "        self.sim_matrix = np.asarray([np.zeros(batch_size) for j in range(batch_size)])\n",
    "        \n",
    "        #Liste des centroids dans le batch = list of np.array \n",
    "        self.centroids = []\n",
    "        \n",
    "        #Taille du batch : int\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #Index du dernier élément supprimé : int\n",
    "        self.last = batch_size\n",
    "    \n",
    "    def fusion(self, merger, to_merge) :\n",
    "        # Effectue la fusion de deux clusteurs en recalculant le centroid puis en fusionnant les set des clusters.\n",
    "        self.centroids[merger] = np.average([self.centroids[merger], self.centroids[to_merge]],\n",
    "                                            weights = [len(self.clusters[merger]), len(self.clusters[to_merge])],\n",
    "                                            axis = 0) \n",
    "        self.clusters[merger] = self.clusters[merger].union(self.clusters[to_merge])\n",
    "\n",
    "    def regen_sim(self, new) :\n",
    "        #Recalcule la matrice de similarité dans la ligne/colonne donnée en argument \n",
    "        k = np.concatenate(cos_sim(self.centroids, self.centroids[new].reshape(1, -1)))\n",
    "        for i, val in enumerate(k) :\n",
    "            self.sim_matrix[i][new] = val\n",
    "        self.sim_matrix[new] = k\n",
    "        self.sim_matrix[new][new] = -1\n",
    "\n",
    "    def return_alone(self) :\n",
    "        # Renvoie aléatoirement un cluster singleton\n",
    "        sorry = []\n",
    "        for ind,clus in self.clusters.items() :\n",
    "            if len(clus) == 1 :\n",
    "                sorry.append((ind, clus.copy().pop()))\n",
    "        shuffle(sorry)\n",
    "        self.last = sorry[0][0]\n",
    "        return sorry[0][1]\n",
    "    \n",
    "    def clusterize(self) : \n",
    "        # Enchaine les processus de fusions :\n",
    "        \"\"\"Trouver la paire avec la plus haute similarité,\n",
    "            Vérifier si la similarité est suffisement élevée,\n",
    "                Effectue la fusion,\n",
    "                Regénère la matrice de similarité,\n",
    "                Memorise l'élément fusionné\n",
    "            Sinon, suprimme un cluster singleton        \n",
    "        \"\"\"\n",
    "        merger, to_merge = np.unravel_index(np.argmax(np.array(self.sim_matrix)), (self.batch_size, self.batch_size))\n",
    "        if self.min_merge < self.sim_matrix[merger][to_merge] :\n",
    "            self.fusion(merger, to_merge)\n",
    "            self.regen_sim(merger)\n",
    "            self.last = to_merge\n",
    "            return None\n",
    "        else :\n",
    "            return self.return_alone()\n",
    "    \n",
    "    def init_hac(self, vector_batch, indexes) :\n",
    "        \n",
    "        # Calcule une matrice de similarité entre les 200\n",
    "        for i in range(self.batch_size) :\n",
    "            self.clusters[i] = {indexes[i]}\n",
    "            self.centroids.append(np.asarray(vector_batch[i]))\n",
    "        self.sim_matrix = cos_sim(self.centroids, self.centroids)\n",
    "        for i in range(self.batch_size) :\n",
    "            self.sim_matrix[i][i] = -1\n",
    "        return self.clusterize()\n",
    "    \n",
    "    def add_vector(self, vector, index) :\n",
    "        \n",
    "        self.clusters[self.last] = {index}\n",
    "        self.centroids[self.last] = np.asarray(vector)\n",
    "        self.regen_sim(self.last)\n",
    "        return self.clusterize()\n",
    "    \n",
    "    def looper(self, indexes, vectors) :\n",
    "        fail = []\n",
    "        k = 1\n",
    "        for i in indexes :\n",
    "            ind = self.add_vector(vectors[i],i)\n",
    "            if ind :\n",
    "                fail.append(ind)\n",
    "            print(\"\\r{:.2%}\".format(k/len(indexes)), end = '')\n",
    "            k+=1\n",
    "        self.min_merge -= self.loss\n",
    "        return fail\n",
    "    \n",
    "    def run_hac(self, vectors) :\n",
    "        order = [i for i in range(len(vectors))]\n",
    "        shuffle(order)\n",
    "        fail = []\n",
    "        ind = self.init_hac([vectors[i] for i in order[:200]],order[:200])\n",
    "        if ind :\n",
    "            fail.append(ind)\n",
    "        fail += self.looper(order[200:], vectors)\n",
    "        i = 1\n",
    "        while i < self.max_passe and fail :\n",
    "            print('\\nEchecs, passe', i, ':', len(fail))\n",
    "            shuffle(fail)\n",
    "            fails = fail.copy()\n",
    "            fail = self.looper(fails, vectors)\n",
    "            i+=1\n",
    "        print('\\nEchecs restants :', len(fail))\n",
    "            \n",
    "        return self.clusters\n",
    "    \n",
    "    def labelise(clusters, data_size) :\n",
    "        labels = [-i-1 for i in range(data_size)]\n",
    "        for i, clus in clusters.items():\n",
    "            for ind in clus :\n",
    "                labels[ind] = i\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase exécutable / de test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage de la metric silhouette**\n",
    "The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.\n",
    "The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### K-means ZONE ####\n",
    "#labels, centroids = run_k_means(good_vecs, nb_cluster)\n",
    "## END K-means ZONE ##\n",
    "\n",
    "### TEST ZONE ##\n",
    "\n",
    "hac = HAC(batch_size = 200, min_merge = 0.50, max_passe = 2, loss = 0.025)\n",
    "\n",
    "%prun hac_clus = HAC.labelise(hac.run_hac(good_vecs), len(good_vecs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic = hac_clus\n",
    "f = open(\"clus.pkl\",\"wb\")\n",
    "pickle.dump(dic,f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
